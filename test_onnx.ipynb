{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b83354-2953-4fd9-94f6-2b16bb408db5",
   "metadata": {},
   "source": [
    "**medium**\n",
    "ModelDimensions(n_mels=80, n_audio_ctx=1500, n_audio_state=1024, n_audio_head=16, n_audio_layer=24, n_vocab=51865, n_text_ctx=448, n_text_state=1024, n_text_head=16, n_text_layer=24)\n",
    "\n",
    "- tokens: torch.Size([5, 1, 1024])\n",
    "- audio_features: torch.Size([5, 1500, 1024])\n",
    "- offset: 0 ++\n",
    "- positional_embedding: torch.Size([1500, 1024])\n",
    "- kv_cache: key:Linear(in_features=1024, out_features=1024, bias=False)  value:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346d368-3823-4790-b0e8-d93cc869cbe6",
   "metadata": {},
   "source": [
    "# No kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103e6951-b417-44ec-b3e8-4256e21d9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"medium\"\n",
    "model = whisper.load_model(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a43f7b7-1025-49fb-be1a-6bc2ebc48301",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels=80\n",
    "n_audio_ctx=1500\n",
    "\n",
    "use_folder = os.path.join(\"/home/mgtv/test_whisper/onnx\", model_name)\n",
    "os.makedirs(use_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d56376a-cba3-4134-9387-4f349f9f9b39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/whisper/model.py:156: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n"
     ]
    }
   ],
   "source": [
    "# encoder input: (batch_size, n_mels, n_ctx)\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model.encoder,\n",
    "        torch.randn(1, n_mels, 3000).to(device),\n",
    "        os.path.join(use_folder, \"whisper_encoder.onnx\"),\n",
    "        opset_version=13,\n",
    "        input_names=['feats'],\n",
    "        output_names=['encoder_out'],\n",
    "        dynamic_axes = {\n",
    "        \"feats\" : {\n",
    "            0: \"batch_size\",\n",
    "        },\n",
    "        \"encoder_out\" : {\n",
    "            0: \"batch_size\",\n",
    "        }}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12359168-ace1-4383-83b4-98ce026e94bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 51865])\n"
     ]
    }
   ],
   "source": [
    "# 没有kv_cache的版本\n",
    "# tokens = torch.tensor([[50258, 50258, 50258, 50258]]).to(device)\n",
    "# tokens = torch.randn(1, 1).long().to(device)\n",
    "tokens = torch.tensor([[50258]]).to(device)\n",
    "audio_feats = torch.randn(1, 1500, 1024).to(device)\n",
    "# cache_size = len(model.decoder.blocks) * 2\n",
    "# kv_cache = torch.randn(cache_size, 1, 4, 1024).to(device)\n",
    "# offset = torch.tensor(0).to(device)\n",
    "# kv_cache[:, :, -1] = 0\n",
    "a = model.decoder(tokens, audio_feats)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa5c5e91-9890-4568-b691-037a82d500d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "    model.decoder,\n",
    "    (tokens, audio_feats),\n",
    "    os.path.join(use_folder, \"whisper_decoder.onnx\"),\n",
    "    verbose=False,\n",
    "    opset_version=13,\n",
    "    input_names = [\"tokens\", \"audio_feats\"],\n",
    "    output_names = [\"logits\"],\n",
    "    dynamic_axes = {\n",
    "        \"tokens\" : {\n",
    "            0: \"batch_size\",\n",
    "            1: \"length\",\n",
    "        },\n",
    "        \"audio_feats\" : {\n",
    "            0: \"batch_size\",\n",
    "        },\n",
    "        \n",
    "        \"logits\" : {\n",
    "            0: \"batch_size\",\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "303756ca-f73b-4fd6-9bb7-a18d44473603",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "basic_filebuf::underflow error reading the file: Is a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8808c9806871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# onnx.checker.check_model(onnx_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/mgtv/test_whisper/onnx/medium/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/onnx/checker.py\u001b[0m in \u001b[0;36mcheck_model\u001b[0;34m(model, full_check)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: (Union[ModelProto, Text], bool) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_model_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: basic_filebuf::underflow error reading the file: Is a directory"
     ]
    }
   ],
   "source": [
    "## 检测模型正确性 模型大于2G无法检测 \n",
    "import onnx \n",
    "\n",
    "onnx_model = onnx.load(\"/home/mgtv/test_whisper/onnx/medium/whisper_decoder.onnx\") \n",
    "onnx.checker.check_model(onnx_model) \n",
    "\n",
    "# try: \n",
    "#     onnx.checker.check_model(onnx_model) \n",
    "# except Exception: \n",
    "#     print(\"Model incorrect\") \n",
    "# else: \n",
    "#     print(\"Model correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5882bab3-19cd-40cb-9c57-b0a04596fcad",
   "metadata": {},
   "source": [
    "## Evaluation No kv_cache\n",
    "使用命令行需要设置fp16=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2283406-fa37-409a-9d65-0c0aa063c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import numpy as np\n",
    "import os\n",
    "import onnxruntime as ort\n",
    "\n",
    "model = whisper.load_model(\"medium\")\n",
    "\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(\"/home/mgtv/test_whisper/test1.mp4\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "# _, probs = model.detect_language(mel)\n",
    "# print(f\"Detected language: {max(probs, key=probs.get)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6101334-7e93-4114-a2f6-becd43636de7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Whisper.encoder of Whisper()>\n",
      "<bound method Whisper.decoder of Whisper()>\n",
      "<bound method decode of Whisper()>\n",
      "True\n",
      "here encoder\n",
      "tensor([[[-0.6236,  0.4615, -0.4376,  ..., -0.0392, -1.2181, -0.2081],\n",
      "         [ 0.9934, -2.4017,  0.9676,  ..., -0.8667, -0.0864, -2.1745],\n",
      "         [ 0.5403, -1.7654,  0.2960,  ..., -1.4299, -0.4214, -3.0424],\n",
      "         ...,\n",
      "         [ 2.0174, -1.1502,  1.1810,  ...,  0.4939,  0.1761, -0.2634],\n",
      "         [ 1.1018, -0.4732,  1.1714,  ...,  0.3203,  0.6978,  0.1125],\n",
      "         [-1.3277,  0.8262,  1.6095,  ...,  1.6831, -0.4086,  0.2371]]])\n",
      "torch.float32\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "# print(model.decoder)\n",
    "# print(model.decode)\n",
    "\n",
    "# print(model.onnx_encoder.run(['encoder_out'], {'feats': mel.cpu().unsqueeze(0).numpy()}))\n",
    "feats = model.encoder(mel.unsqueeze(0))\n",
    "print(feats)\n",
    "print(feats[0].dtype)\n",
    "print(feats[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9702f6-55ad-48d4-a693-7b3f3173ad74",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here encoder\n",
      "shape of run get audio_features:********************\n",
      "torch.Size([1, 1500, 1024])\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "here decoder\n",
      "DecodingResult(audio_features=tensor([[-0.6236,  0.4615, -0.4376,  ..., -0.0392, -1.2181, -0.2081],\n",
      "        [ 0.9934, -2.4017,  0.9676,  ..., -0.8667, -0.0864, -2.1745],\n",
      "        [ 0.5403, -1.7654,  0.2960,  ..., -1.4299, -0.4214, -3.0424],\n",
      "        ...,\n",
      "        [ 2.0174, -1.1502,  1.1810,  ...,  0.4939,  0.1761, -0.2634],\n",
      "        [ 1.1018, -0.4732,  1.1714,  ...,  0.3203,  0.6978,  0.1125],\n",
      "        [-1.3277,  0.8262,  1.6095,  ...,  1.6831, -0.4086,  0.2371]]), language='zh', language_probs=None, tokens=[50364, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260], text='<|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|>', avg_logprob=-0.0002133827739291721, no_speech_prob=0.0016939222114160657, temperature=0.0, compression_ratio=53.52)\n",
      "<|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|>\n"
     ]
    }
   ],
   "source": [
    "# decode the audio\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "358dc3ce-4658-448f-b9fe-1dd974b4eb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3293bda-c3ea-40d0-84d2-be8178a05fae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "False\n",
      "shape of decoding.run() audio_features:********************\n",
      "torch.Size([1, 1500, 1024])\n",
      "torch.float32\n",
      "False\n",
      "shape of decoding.run() audio_features:********************\n",
      "torch.Size([1, 1500, 1024])\n",
      "torch.float32\n",
      "False\n",
      "shape of decoding.run() audio_features:********************\n",
      "torch.Size([1, 1500, 1024])\n",
      "torch.float32\n",
      "False\n",
      "shape of decoding.run() audio_features:********************\n",
      "torch.Size([1, 1500, 1024])\n",
      "process spent 21.57512140274048 s. \n",
      "\n",
      "decode spent 21.56056809425354 s. \n",
      "\n",
      "{'text': '本身我会 | 但是下赛道是第一次 | 那个很帅 | 很刺激 | 我也想要刺激 | 太厉害了 | 2018年演了一部音乐剧 | 那部音乐剧是 | 韩瘋老师跟田庆星导演 | 做的一部音乐剧 | 然后那部音乐剧是让我 | 就是两个月跟大家待在一起 | 然后我那个剧演的是 | 爱情的主题 | 所以就是跟那个男主角 | 就是每天就是 | 就会就靠得很近 | 然后因为我们 | 就是那个剧里面 | 心都亲了十次 | 然后我就觉得通过那部剧 | 就是让我心整个一下就 | 打开了一下 | 打开了 | 就跟之前就完全不一样 | 对 改变了我的性格是不是 | 所以2018年的这个 | 知否知否也是对于我来说 | 非常重要的一首歌 | 那我们来看看 | 这封信是谁写的吧 | 可为你好 | 这三个月的时光音乐之旅 | 让我们彼此都非常幸福', 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 0.8, 'text': '本身我会', 'tokens': [8802, 19847, 1654, 12949], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 1, 'seek': 0, 'start': 0.8, 'end': 2.36, 'text': '但是下赛道是第一次', 'tokens': [11189, 4438, 5266, 249, 6025, 1541, 18049, 9487], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 2, 'seek': 0, 'start': 3.2, 'end': 4.48, 'text': '那个很帅', 'tokens': [43083, 4563, 4845, 227], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 3, 'seek': 0, 'start': 4.48, 'end': 5.04, 'text': '很刺激', 'tokens': [4563, 2437, 118, 26373, 222], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 4, 'seek': 0, 'start': 5.04, 'end': 6.2, 'text': '我也想要刺激', 'tokens': [27761, 7093, 4275, 2437, 118, 26373, 222], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 5, 'seek': 0, 'start': 6.2, 'end': 7.32, 'text': '太厉害了', 'tokens': [9455, 5014, 231, 14694, 2289], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 6, 'seek': 0, 'start': 8.32, 'end': 10.72, 'text': '2018年演了一部音乐剧', 'tokens': [35023, 5157, 31382, 2289, 2257, 13470, 18034, 44365, 5935, 100], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 7, 'seek': 0, 'start': 10.72, 'end': 11.92, 'text': '那部音乐剧是', 'tokens': [4184, 13470, 18034, 44365, 5935, 100, 1541], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 8, 'seek': 0, 'start': 11.92, 'end': 15.120000000000001, 'text': '韩瘋老师跟田庆星导演', 'tokens': [13665, 102, 163, 246, 233, 10439, 29186, 9678, 24727, 6346, 228, 20682, 4510, 120, 31382], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 9, 'seek': 0, 'start': 15.120000000000001, 'end': 16.48, 'text': '做的一部音乐剧', 'tokens': [10907, 1546, 2257, 13470, 18034, 44365, 5935, 100], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 10, 'seek': 0, 'start': 16.48, 'end': 17.8, 'text': '然后那部音乐剧是让我', 'tokens': [26636, 4184, 13470, 18034, 44365, 5935, 100, 1541, 33650, 1654], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 11, 'seek': 0, 'start': 17.8, 'end': 19.84, 'text': '就是两个月跟大家待在一起', 'tokens': [5620, 36257, 7549, 6939, 47349, 18390, 3581, 29567], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 12, 'seek': 0, 'start': 19.84, 'end': 21.12, 'text': '然后我那个剧演的是', 'tokens': [26636, 1654, 43083, 5935, 100, 31382, 24620], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 13, 'seek': 0, 'start': 21.12, 'end': 22.6, 'text': '爱情的主题', 'tokens': [27324, 10570, 1546, 13557, 30716], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 14, 'seek': 0, 'start': 22.6, 'end': 24.080000000000002, 'text': '所以就是跟那个男主角', 'tokens': [7239, 5620, 9678, 43083, 25243, 13557, 29389], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 15, 'seek': 0, 'start': 24.080000000000002, 'end': 25.12, 'text': '就是每天就是', 'tokens': [5620, 23664, 6135, 5620], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 16, 'seek': 0, 'start': 25.12, 'end': 26.6, 'text': '就会就靠得很近', 'tokens': [3111, 12949, 3111, 5363, 254, 5916, 4563, 17463], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 17, 'seek': 0, 'start': 26.6, 'end': 27.68, 'text': '然后因为我们', 'tokens': [26636, 34627, 15003], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 18, 'seek': 0, 'start': 27.68, 'end': 28.6, 'text': '就是那个剧里面', 'tokens': [5620, 43083, 5935, 100, 15759, 8833], 'temperature': 0.0, 'avg_logprob': -0.20897110163816932, 'compression_ratio': 1.4870848708487086, 'no_speech_prob': 0.0016885249642655253}, {'id': 19, 'seek': 2860, 'start': 28.6, 'end': 30.68, 'text': '心都亲了十次', 'tokens': [7945, 7182, 49296, 2289, 20145, 9487], 'temperature': 0.0, 'avg_logprob': -0.23410820750008643, 'compression_ratio': 1.2479338842975207, 'no_speech_prob': 1.392946251144167e-05}, {'id': 20, 'seek': 2860, 'start': 30.68, 'end': 32.24, 'text': '然后我就觉得通过那部剧', 'tokens': [26636, 22020, 29992, 19550, 16866, 4184, 13470, 5935, 100], 'temperature': 0.0, 'avg_logprob': -0.23410820750008643, 'compression_ratio': 1.2479338842975207, 'no_speech_prob': 1.392946251144167e-05}, {'id': 21, 'seek': 2860, 'start': 32.24, 'end': 34.08, 'text': '就是让我心整个一下就', 'tokens': [5620, 33650, 1654, 7945, 27662, 7549, 8861, 3111], 'temperature': 0.0, 'avg_logprob': -0.23410820750008643, 'compression_ratio': 1.2479338842975207, 'no_speech_prob': 1.392946251144167e-05}, {'id': 22, 'seek': 2860, 'start': 34.08, 'end': 34.68, 'text': '打开了一下', 'tokens': [12467, 18937, 2289, 8861], 'temperature': 0.0, 'avg_logprob': -0.23410820750008643, 'compression_ratio': 1.2479338842975207, 'no_speech_prob': 1.392946251144167e-05}, {'id': 23, 'seek': 2860, 'start': 34.68, 'end': 35.4, 'text': '打开了', 'tokens': [12467, 18937, 2289], 'temperature': 0.0, 'avg_logprob': -0.23410820750008643, 'compression_ratio': 1.2479338842975207, 'no_speech_prob': 1.392946251144167e-05}, {'id': 24, 'seek': 2860, 'start': 35.4, 'end': 37.36, 'text': '就跟之前就完全不一样', 'tokens': [3111, 9678, 32442, 3111, 37100, 1960, 2257, 14496], 'temperature': 0.0, 'avg_logprob': -0.23410820750008643, 'compression_ratio': 1.2479338842975207, 'no_speech_prob': 1.392946251144167e-05}, {'id': 25, 'seek': 2860, 'start': 37.36, 'end': 39.64, 'text': '对 改变了我的性格是不是', 'tokens': [8713, 220, 34490, 2129, 246, 2289, 14200, 21686, 30921, 23034], 'temperature': 0.0, 'avg_logprob': -0.23410820750008643, 'compression_ratio': 1.2479338842975207, 'no_speech_prob': 1.392946251144167e-05}, {'id': 26, 'seek': 2860, 'start': 39.64, 'end': 42.160000000000004, 'text': '所以2018年的这个', 'tokens': [7239, 35023, 34128, 15368], 'temperature': 0.0, 'avg_logprob': -0.23410820750008643, 'compression_ratio': 1.2479338842975207, 'no_speech_prob': 1.392946251144167e-05}, {'id': 27, 'seek': 2860, 'start': 42.160000000000004, 'end': 44.08, 'text': '知否知否也是对于我来说', 'tokens': [6498, 45334, 6498, 45334, 22021, 8713, 37732, 1654, 6912, 8090], 'temperature': 0.0, 'avg_logprob': -0.23410820750008643, 'compression_ratio': 1.2479338842975207, 'no_speech_prob': 1.392946251144167e-05}, {'id': 28, 'seek': 2860, 'start': 44.08, 'end': 46.0, 'text': '非常重要的一首歌', 'tokens': [14392, 24928, 1546, 2257, 25444, 29582], 'temperature': 0.0, 'avg_logprob': -0.23410820750008643, 'compression_ratio': 1.2479338842975207, 'no_speech_prob': 1.392946251144167e-05}, {'id': 29, 'seek': 2860, 'start': 46.0, 'end': 47.24, 'text': '那我们来看看', 'tokens': [4184, 15003, 6912, 22884], 'temperature': 0.0, 'avg_logprob': -0.23410820750008643, 'compression_ratio': 1.2479338842975207, 'no_speech_prob': 1.392946251144167e-05}, {'id': 30, 'seek': 2860, 'start': 47.24, 'end': 49.08, 'text': '这封信是谁写的吧', 'tokens': [5562, 1530, 223, 17665, 1541, 33556, 5676, 247, 1546, 6062], 'temperature': 0.0, 'avg_logprob': -0.23410820750008643, 'compression_ratio': 1.2479338842975207, 'no_speech_prob': 1.392946251144167e-05}, {'id': 31, 'seek': 2860, 'start': 55.84, 'end': 56.8, 'text': '可为你好', 'tokens': [4429, 13992, 26410], 'temperature': 0.0, 'avg_logprob': -0.23410820750008643, 'compression_ratio': 1.2479338842975207, 'no_speech_prob': 1.392946251144167e-05}, {'id': 32, 'seek': 5680, 'start': 56.8, 'end': 58.8, 'text': '这三个月的时光音乐之旅', 'tokens': [5562, 10960, 7549, 6939, 1546, 15729, 20690, 18034, 44365, 9574, 4479, 227], 'temperature': 0.0, 'avg_logprob': -0.1555888851483663, 'compression_ratio': 0.8382352941176471, 'no_speech_prob': 9.131765546044335e-05}, {'id': 33, 'seek': 5880, 'start': 58.8, 'end': 86.8, 'text': '让我们彼此都非常幸福', 'tokens': [50364, 33650, 15003, 7391, 120, 17947, 7182, 14392, 36482, 33952, 51764], 'temperature': 0.0, 'avg_logprob': -0.518764058748881, 'compression_ratio': 0.7317073170731707, 'no_speech_prob': 0.006587798707187176}], 'language': 'zh'}\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(\"/home/mgtv/test_whisper/test1.mp4\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc125032-f5e0-42a9-a35f-d242c2d5c0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load original checkpoint\n",
      "process spent 9.733283042907715 s. \n",
      "\n",
      "decode spent 9.724674224853516 s. \n",
      "\n",
      "本身我会但是下赛道是第一次那个很帅很刺激我也想要刺激太厉害了2018年演了一部音乐剧那部音乐剧是韩瘋老师跟田庆星导演做的一部音乐剧然后那部音乐剧是让我就是两个月跟大家待在一起然后我那个剧演的是爱情的主题所以就是跟那个男主角就是每天就是就会就靠得很近然后因为我们就是那个剧里面心都亲了十次然后我就觉得通过那部剧就是让我心整个一下就打开了一下打开了就跟之前就完全不一样对 改变了我的性格是不是所以2018年的这个知否知否也是对于我来说非常重要的一首歌那我们来看看这封信是谁写的吧可为你好这三个月的时光音乐之旅让我们彼此都非常幸福\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"medium\")\n",
    "result = model.transcribe(\"/home/mgtv/test_whisper/test1.mp4\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da63be47-519d-498c-b473-5bd3cf1bdef8",
   "metadata": {},
   "source": [
    "# With kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6012c74-c657-40ce-acec-bb8658f7bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"medium\"\n",
    "model = whisper.load_model(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b972df3-bd88-487d-a806-5b565ffdee71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff0c65a-bf45-47ff-993b-ffaf44c1ee9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "True\n",
      "shape of decoding.run() audio_features:********************\n",
      "torch.Size([1, 1500, 1024])\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(\"/home/mgtv/test_whisper/test1.mp4\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f418b5c-bf3b-4e4c-bc26-0e2213cb613d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(\"/home/mgtv/test_whisper/test1.mp4\")\n",
    "print(1)\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "print(2)\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "print(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5602a-2c86-46fa-b3b6-181aa6b604a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "True\n",
      "shape of decoding.run() audio_features:********************\n",
      "torch.Size([1, 1500, 1024])\n"
     ]
    }
   ],
   "source": [
    "# decode the audio\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e13a0a0-5132-4301-b809-7505576b5ca0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-641c95d8cb76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# kv_cache[:, :, -1] = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0maudio\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mattended\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \"\"\"\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkv_cache\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "cache_size = len(model.decoder.blocks) * 2\n",
    "tokens = torch.tensor([[50258, 50258, 50258, 50258]]).to(device)\n",
    "audio_feats = torch.randn(1, 1500, 1024).to(device)\n",
    "kv_cache = torch.randn(cache_size, 1, 4, 1024).to(device)\n",
    "offset = torch.tensor(0).to(device)\n",
    "# kv_cache[:, :, -1] = 0\n",
    "a, new_cache = model.decoder(tokens, audio_feats, kv_cache)\n",
    "print(a.shape)\n",
    "print(new_cache.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dbfa4ed-71cc-4eff-9f40-ec0c6730d131",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3e74c83e3c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkv_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/whisper/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \"\"\"\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0  原版\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mkv_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "tokens = torch.tensor([[50258]]).to(device)\n",
    "audio_feats = torch.randn(1, 1500, 1024).to(device)\n",
    "cache_size = len(model.decoder.blocks) * 2\n",
    "kv_cache = torch.randn(cache_size, 1, 5, 1024).to(device)\n",
    "offset = torch.tensor(4).to(device)\n",
    "a = model.decoder(tokens, audio_feats, kv_cache)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d8ba5-c60c-4752-a24c-30fcae5409fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "    this_model.decoder,\n",
    "    (tokens, audio_feats, kv_cache, offset),\n",
    "    os.path.join(use_folder, \"whisper_decoder.onnx\"),\n",
    "    verbose=False,\n",
    "    opset_version=14,\n",
    "    input_names = [\"tokens\", \"audio_feats\"],\n",
    "    output_names = [\"logits\"],\n",
    "    dynamic_axes = {\n",
    "        \"tokens\" : {\n",
    "            0: \"batch_size\",\n",
    "            1: \"length\",\n",
    "        },\n",
    "        \"audio_feats\" : {\n",
    "            0: \"batch_size\",\n",
    "        },\n",
    "        \"kv_cache\" : [1, 2],\n",
    "        \"logits\" : {\n",
    "            0: \"batch_size\",\n",
    "        },\n",
    "        \"output_kv_cache\" : [1, 2],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3053cb-9862-4a97-8e0c-f46b3c3352d9",
   "metadata": {},
   "source": [
    "### Some Ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1d33e-065f-46d7-b704-ae5c37ed5777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 知乎超分示例\n",
    "import onnxruntime \n",
    " \n",
    "ort_session = onnxruntime.InferenceSession(\"srcnn.onnx\") \n",
    "ort_inputs = {'input': input_img} \n",
    "ort_output = ort_session.run(['output'], ort_inputs)[0] \n",
    " \n",
    "ort_output = np.squeeze(ort_output, 0) \n",
    "ort_output = np.clip(ort_output, 0, 255) \n",
    "ort_output = np.transpose(ort_output, [1, 2, 0]).astype(np.uint8) \n",
    "cv2.imwrite(\"face_ort.png\", ort_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d87c1de-af6b-4687-ae41-043be58bd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe8573-cb10-44d5-93a4-41535d635d2d",
   "metadata": {},
   "source": [
    "providers = [\"CUDAExecutionProvider\"]  默认CPUExecutionProvider\n",
    "TensorrtExecutionProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2cae4b-06de-40cd-9053-b388549406da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperOnnx:\n",
    "    SAMPLERATE = 16000\n",
    "    MAX_DURATION = 30\n",
    "    N_FFT = 400\n",
    "    N_MELS = 80\n",
    "    HOP_LENGTH = 160\n",
    "    MAX_FRAMES = 3000\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dir = None,\n",
    "        providers = [\"CUDAExecutionProvider\"],\n",
    "    ): \n",
    "        self.encoder = ort.InferenceSession(\n",
    "            os.path.join(model_dir, 'whisper_encoder.onnx'),\n",
    "            providers = providers,\n",
    "        )\n",
    "        self.decoder = ort.InferenceSession(\n",
    "            os.path.join(model_dir, 'whisper_decoder.onnx'),\n",
    "            providers = providers,\n",
    "        )\n",
    "        \n",
    "        self.max_samples = self.SAMPLERATE * self.MAX_DURATION\n",
    "        self.tokenizer = get_tokenizer(False, language=\"en\", task=\"transcribe\")\n",
    "    \n",
    "ort_inputs = {'input': input_img} \n",
    "ort_output = ort_session.run(['output'], ort_inputs)[0]\n",
    "ws2t = WhisperOnnx(model_dir=\"/home/mgtv/test_whisper/onnx/medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54899768-aee8-4d4d-963a-8adbcee229eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(3 << 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2f764-da4f-49aa-a3a4-fbbd73bb61db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation of ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63cc1db-0dc2-4bed-bd26-1b1cbcbb39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import onnxruntime as ort\n",
    "from hyperpyyaml import load_hyperpyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da0b1e-cead-4e45-851a-e8dd8d168c93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Stft:\n",
    "    \"\"\"STFT module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fft=400,\n",
    "        win_length=400,\n",
    "        hop_length=160,\n",
    "        center=True,\n",
    "        window=\"hann\",\n",
    "        onesided = True,\n",
    "        normalized = False,\n",
    "        pad_mode = \"reflect\",\n",
    "    ):\n",
    "        self.stft_kwargs = dict(\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            center=center,\n",
    "            window=window,\n",
    "            pad_mode=pad_mode,\n",
    "        )\n",
    "        self.onesided = onesided\n",
    "        self.normalized = normalized\n",
    "\n",
    "    def __call__(\n",
    "        self, input: np.ndarray, ilens: np.ndarray = None\n",
    "    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        \"\"\"STFT forward function.\n",
    "        Args:\n",
    "            input: (Batch, Nsamples)\n",
    "            ilens: (Batch)\n",
    "        Returns:\n",
    "            output: (Batch, Frames, Freq, 2)\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        # iterate over istances in a batch\n",
    "        for i, instance in enumerate(input):\n",
    "            stft = librosa.stft(input[i], **self.stft_kwargs)\n",
    "            output.append(np.array(np.stack([stft.real, stft.imag], -1)))\n",
    "        output = np.vstack(output).reshape(len(output), *output[0].shape)\n",
    "\n",
    "        if not self.onesided:\n",
    "            len_conj = self.n_fft - output.shape[1]\n",
    "            conj = output[:, 1: 1 + len_conj].flip(1)\n",
    "            conj[:, :, :, -1].data *= -1\n",
    "            output = np.concatenate([output, conj], 1)\n",
    "        if self.normalized:\n",
    "            output = output * (self.stft_kwargs[\"window\"].shape[0] ** (-0.5))\n",
    "\n",
    "        # output: (Batch, Freq, Frames, 2=real_imag)\n",
    "        # -> (Batch, Frames, Freq, 2=real_imag)\n",
    "        output = output.transpose(0, 2, 1, 3)\n",
    "        olens = None\n",
    "        \n",
    "        # create complex array\n",
    "        output = output[..., 0] + output[..., 1] * 1j\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b0459-4024-42ff-83a5-0a5f88ede514",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogMel:\n",
    "    \"\"\"Convert STFT to fbank feats\n",
    "    The arguments is same as librosa.filters.mel\n",
    "    Args:\n",
    "        config.fs: number > 0 [scalar] sampling rate of the incoming signal\n",
    "        config.n_fft: int > 0 [scalar] number of FFT components\n",
    "        config.n_mels: int > 0 [scalar] number of Mel bands to generate\n",
    "        config.fmin: float >= 0 [scalar] lowest frequency (in Hz)\n",
    "        config.fmax: float >= 0 [scalar] highest frequency (in Hz).\n",
    "            If `None`, use `fmax = fs / 2.0`\n",
    "        config.htk: use HTK formula instead of Slaney\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fmin = 0,\n",
    "        fmax = 8000,\n",
    "        sr = 16000,\n",
    "        n_fft = 400,\n",
    "        n_mels = 80,\n",
    "        htk = False,\n",
    "        log_base = 10.0,\n",
    "    ):\n",
    "        fmin = 0 if fmin is None else fmin\n",
    "        fmax = sr / 2 if fmax is None else fmax\n",
    "        _mel_options = dict(\n",
    "            sr=sr,\n",
    "            n_fft=n_fft,\n",
    "            n_mels=n_mels,\n",
    "            fmin=fmin,\n",
    "            fmax=fmax,\n",
    "            htk=htk,\n",
    "        )\n",
    "        self.mel_options = _mel_options\n",
    "        self.log_base = log_base\n",
    "        melmat = librosa.filters.mel(**_mel_options)\n",
    "        self.melmat = melmat.T\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \", \".join(f\"{k}={v}\" for k, v in self.mel_options.items())\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        feat: np.ndarray,\n",
    "        ilens: np.ndarray = None,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        # feat: (B, T, D1) x melmat: (D1, D2) -> mel_feat: (B, T, D2)\n",
    "        mel_feat = np.matmul(feat, self.melmat)\n",
    "        mel_feat = np.clip(mel_feat, 1e-10, float('inf'))\n",
    "\n",
    "        if self.log_base is None:\n",
    "            logmel_feat = np.log(mel_feat)\n",
    "        elif self.log_base == 2.0:\n",
    "            logmel_feat = np.log2(mel_feat)\n",
    "        elif self.log_base == 10.0:\n",
    "            logmel_feat = np.log10(mel_feat)\n",
    "        else:\n",
    "            logmel_feat = np.log(mel_feat) / np.log(self.log_base)\n",
    "        \n",
    "        # Zero padding\n",
    "        return logmel_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d90bd-fdc4-4c4f-9787-6aa7f97cc325",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WhisperSpeech2Text:\n",
    "    SAMPLERATE = 16000\n",
    "    MAX_DURATION = 30\n",
    "    N_FFT = 400\n",
    "    N_MELS = 80\n",
    "    HOP_LENGTH = 160\n",
    "    MAX_FRAMES = 3000\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tag_name: str = None,\n",
    "        model_dir = None,\n",
    "        providers = [\"CPUExecutionProvider\"],\n",
    "        use_quantized: bool = False,\n",
    "    ):\n",
    "        sess_options = ort.SessionOptions()\n",
    "        sess_options.inter_op_num_threads = 1\n",
    "        sess_options.intra_op_num_threads = 1\n",
    "\n",
    "        overrides = dict(\n",
    "            root_dir=model_dir\n",
    "        )\n",
    "        with open(os.path.join(model_dir, \"config.yaml\"), encoding=\"utf-8\") as reader:\n",
    "            config = load_hyperpyyaml(reader, overrides)\n",
    "        \n",
    "        self._config = config\n",
    "        self.encoder = ort.InferenceSession(\n",
    "            config[\"encoder\"][\"model_path\"],\n",
    "            providers = providers,\n",
    "            sess_options = sess_options\n",
    "        )\n",
    "        self.decoder = ort.InferenceSession(\n",
    "            config[\"decoder\"][\"model_path\"],\n",
    "            providers = providers,\n",
    "            sess_options = sess_options\n",
    "        )\n",
    "        self.max_samples = self.SAMPLERATE * self.MAX_DURATION\n",
    "        self.stft = Stft()\n",
    "        self.logmel = LogMel()\n",
    "        self.tokenizer = get_tokenizer(False, language=\"en\", task=\"transcribe\")\n",
    "    \n",
    "    def __call__(self, xs_tensor: np.array):\n",
    "        if xs_tensor.ndim > 2:\n",
    "            raise ValueError(\"Only single channels is allowed.\")\n",
    "        if xs_tensor.shape[0] > self.max_samples:\n",
    "            xs_tensor = xs_tensor[:self.max_samples]\n",
    "        elif xs_tensor.shape[0] < self.max_samples:\n",
    "            pads = self.max_samples - xs_tensor.shape[0]\n",
    "            xs_tensor = np.pad(xs_tensor, (0, pads), 'constant')\n",
    "        xs_tensor = self.get_feats(xs_tensor[None])\n",
    "        hs_tensor, = self.encoder.run([\"output\"], {\"input\" : xs_tensor})\n",
    "        return hs_tensor\n",
    "\n",
    "    def get_feats(self, xs_tensor: np.array):\n",
    "        xs_tensor = self.stft(xs_tensor)[:, :self.MAX_FRAMES]\n",
    "        xs_tensor = xs_tensor.real ** 2 + xs_tensor.imag ** 2\n",
    "        xs_tensor = self.logmel(xs_tensor)\n",
    "        # Whisper Normalization:\n",
    "        amax = np.amax(xs_tensor) - 8.0\n",
    "        xs_tensor = np.clip(xs_tensor, None, amax)\n",
    "        xs_tensor = (xs_tensor + 4.0) / 4.0\n",
    "        return np.swapaxes(xs_tensor, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf529e-30d7-45f5-85dc-8e1d95776dce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ws2t = WhisperSpeech2Text(model_dir=\"../build/tiny\")\n",
    "\n",
    "print(ws2t._config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911ee47-00c6-48e5-91f4-d66e89856c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    signal = np.random.randn(16000 * 30).astype(np.float32)\n",
    "    hs_tensor = ws2t(signal)\n",
    "    # start_id = np.array([[50258, 5, 6, 2, 6, 1, 5, 1, 6]], dtype=int)\n",
    "    # ys_tensor, = ws2t.decoder.run([\"logits\"], {\"input_text\" : start_id, \"input_speech\": hs_tensor})\n",
    "    # print(ys_tensor)\n",
    "    print(hs_tensor.shape)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d8e3a-75b5-40e7-a24d-93dfb4f71bea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = ws2t.tokenizer.encode(\"<|endoftext|>I would like to know about how this is workidns<|endoftext|>\")\n",
    "print(tokens)\n",
    "text = ws2t.tokenizer.decode(tokens)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f0044-9359-43d6-afb8-ea8980f97818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564dadb4-f612-4143-9ece-522aaf4b825a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
